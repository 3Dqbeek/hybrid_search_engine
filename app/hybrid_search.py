"""
–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç BM25, Semantic Search, Keyword Density –∏ Context Boost
"""

import logging
import math
import re
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter
import numpy as np
from elasticsearch import Elasticsearch
import requests
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QueryAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
    
    def __init__(self, llm_url: str):
        self.llm_url = llm_url
    
    def analyze(self, query: str) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        keywords = self._extract_keywords(query)
        intent = self._detect_intent(query)
        entities = self._extract_entities(query)
        
        # LLM —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        llm_analysis = self._llm_expand(query)
        
        return {
            'original_query': query,
            'keywords': keywords,
            'intent': intent,
            'entities': entities,
            'expanded_query': llm_analysis.get('enhanced_query', query),
            'concepts': llm_analysis.get('concepts', []),
            'query_type': self._classify_query_type(query, intent)
        }
    
    def _extract_keywords(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–ø–æ–∫–∞–∂–∏', '–Ω–∞–π–¥–∏', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '–¥–∏–∞–ª–æ–≥–∏', '–∑–≤–æ–Ω–∫–∏', '—Ä–∞–∑–≥–æ–≤–æ—Ä—ã'}
        words = query.lower().split()
        keywords = [w for w in words if w not in stop_words and len(w) > 2]
        return keywords
    
    def _detect_intent(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        query_lower = query.lower()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è intent
        if any(w in query_lower for w in ['–≤—Ö–æ–¥—è—â', '–≤—Ö–æ–¥']):
            return '–≤—Ö–æ–¥—è—â–∏–µ_–∑–≤–æ–Ω–∫–∏'
        elif any(w in query_lower for w in ['–Ω–µ–¥–æ–≤–æ–ª—å', '–∂–∞–ª–æ–±', '–ø—Ä–æ–±–ª–µ–º']):
            return '–Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ_–∫–ª–∏–µ–Ω—Ç–∞'
        elif any(w in query_lower for w in ['–æ–ø–µ—Ä–∞—Ç–æ—Ä', '–º–µ–Ω–µ–¥–∂–µ—Ä']) and any(w in query_lower for w in ['–≥—Ä—É–±', '—Ö–∞–º', '–Ω–µ–≤–µ–∂–ª–∏–≤']):
            return '–ø—Ä–æ–±–ª–µ–º—ã_—Å_–æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º'
        elif any(w in query_lower for w in ['–ø—Ä–æ–¥–∞', '–∫—É–ø', '–∑–∞–∫–∞–∑']):
            return '–ø—Ä–æ–¥–∞–∂–∏'
        elif any(w in query_lower for w in ['—ç–º–ø–∞—Ç', '–¥–æ–≤–æ–ª–µ–Ω', '–¥–æ–≤–æ–ª—å']):
            return '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ_—ç–º–æ—Ü–∏–∏'
        else:
            return '–æ–±—â–∏–π_–ø–æ–∏—Å–∫'
    
    def _extract_entities(self, query: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        entities = {
            'operators': [],
            'products': [],
            'emotions': [],
            'call_types': []
        }
        
        query_lower = query.lower()
        
        # –¢–∏–ø—ã –∑–≤–æ–Ω–∫–æ–≤
        if '–≤—Ö–æ–¥—è—â' in query_lower:
            entities['call_types'].append('–í—Ö–æ–¥—è—â–∏–π –∑–≤–æ–Ω–æ–∫')
        if '–∏—Å—Ö–æ–¥—è—â' in query_lower:
            entities['call_types'].append('–ò—Å—Ö–æ–¥—è—â–∏–π –∑–≤–æ–Ω–æ–∫')
        
        # –≠–º–æ—Ü–∏–∏
        emotion_keywords = {
            '–Ω–µ–¥–æ–≤–æ–ª—å': 'negative',
            '–∂–∞–ª–æ–±': 'negative',
            '–≥—Ä—É–±': 'negative',
            '–¥–æ–≤–æ–ª–µ–Ω': 'positive',
            '—ç–º–ø–∞—Ç': 'positive'
        }
        
        for word, emotion in emotion_keywords.items():
            if word in query_lower:
                entities['emotions'].append(emotion)
        
        return entities
    
    def _classify_query_type(self, query: str, intent: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        if intent in ['–≤—Ö–æ–¥—è—â–∏–µ_–∑–≤–æ–Ω–∫–∏']:
            return 'structured'  # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å (—Ñ–∏–ª—å—Ç—Ä—ã)
        elif intent in ['–Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ_–∫–ª–∏–µ–Ω—Ç–∞', '–ø—Ä–æ–±–ª–µ–º—ã_—Å_–æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º']:
            return 'semantic'  # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π (–Ω—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        else:
            return 'keyword'  # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    
    def _llm_expand(self, query: str) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ LLM"""
        try:
            prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –¥–∏–∞–ª–æ–≥–∞—Ö –∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä–∞: "{query}"

–ò–∑–≤–ª–µ–∫–∏:
1. –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ (–Ω–µ —Å–ª–æ–≤–∞!)
2. –°–∏–Ω–æ–Ω–∏–º—ã
3. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã

–û—Ç–≤–µ—Ç—å JSON:
{{
    "enhanced_query": "—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
    "concepts": ["–∫–æ–Ω—Ü–µ–ø—Ü–∏—è1", "–∫–æ–Ω—Ü–µ–ø—Ü–∏—è2"]
}}
"""
            response = requests.post(
                f"{self.llm_url}/v1/chat/completions",
                json={
                    "model": "qwen/qwen3-coder-30b",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 200,
                    "temperature": 0.2
                },
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result['choices'][0]['message']['content'].strip()
                
                # –ü–∞—Ä—Å–∏–º JSON
                if '```json' in llm_response:
                    llm_response = llm_response.split('```json')[1].split('```')[0].strip()
                elif '```' in llm_response:
                    llm_response = llm_response.split('```')[1].split('```')[0].strip()
                
                return json.loads(llm_response)
        except Exception as e:
            logger.warning(f"LLM —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: {e}")
        
        # Fallback
        return {
            "enhanced_query": query,
            "concepts": []
        }


class KeywordDensityScorer:
    """–ü–æ–¥—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
    
    def score(self, query_words: List[str], text: str, metadata: Dict[str, Any] = None) -> Dict[str, float]:
        """
        –ü–æ–¥—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - density_score: –æ–±—â–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        - tf_scores: TF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        - proximity_bonus: –±–æ–Ω—É—Å –∑–∞ –±–ª–∏–∑–æ—Å—Ç—å —Å–ª–æ–≤
        - position_bonus: –±–æ–Ω—É—Å –∑–∞ –ø–æ–∑–∏—Ü–∏—é
        """
        if not text or not query_words:
            return {
                'density_score': 0.0,
                'tf_scores': {},
                'proximity_bonus': 1.0,
                'position_bonus': 1.0
            }
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        text_lower = text.lower()
        text_words = re.findall(r'\w+', text_lower)
        total_words = len(text_words)
        
        if total_words == 0:
            return {
                'density_score': 0.0,
                'tf_scores': {},
                'proximity_bonus': 1.0,
                'position_bonus': 1.0
            }
        
        # TF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        tf_scores = {}
        word_positions = {}
        
        for query_word in query_words:
            word_lower = query_word.lower()
            count = text_words.count(word_lower)
            
            # TF (Term Frequency) —Å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
            tf = count / total_words
            tf_log = 1 + math.log(1 + tf * total_words) if tf > 0 else 0
            
            tf_scores[query_word] = tf_log
            
            # –ü–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
            positions = [i for i, w in enumerate(text_words) if w == word_lower]
            word_positions[query_word] = positions
        
        # –û–±—â–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
        density_score = sum(tf_scores.values()) / len(query_words) if query_words else 0.0
        
        # Proximity Bonus (–±–ª–∏–∑–æ—Å—Ç—å —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞)
        proximity_bonus = self._calculate_proximity_bonus(query_words, word_positions)
        
        # Position Bonus (–ø–æ–∑–∏—Ü–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ)
        position_bonus = self._calculate_position_bonus(query_words, word_positions, total_words)
        
        return {
            'density_score': density_score,
            'tf_scores': tf_scores,
            'proximity_bonus': proximity_bonus,
            'position_bonus': position_bonus
        }
    
    def _calculate_proximity_bonus(self, query_words: List[str], word_positions: Dict[str, List[int]]) -> float:
        """–ë–æ–Ω—É—Å –∑–∞ –±–ª–∏–∑–æ—Å—Ç—å —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞"""
        if len(query_words) < 2:
            return 1.0
        
        min_distance = float('inf')
        
        # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏ –∑–∞–ø—Ä–æ—Å–∞
        for i, word1 in enumerate(query_words):
            for word2 in query_words[i+1:]:
                if word1 in word_positions and word2 in word_positions:
                    for pos1 in word_positions[word1]:
                        for pos2 in word_positions[word2]:
                            distance = abs(pos1 - pos2)
                            min_distance = min(min_distance, distance)
        
        if min_distance == float('inf'):
            return 1.0
        
        # –ë–æ–Ω—É—Å –∑–∞ –±–ª–∏–∑–æ—Å—Ç—å
        if min_distance <= 3:
            return 3.0  # –°–ª–æ–≤–∞ –æ—á–µ–Ω—å –±–ª–∏–∑–∫–æ
        elif min_distance <= 5:
            return 2.0  # –ë–ª–∏–∑–∫–æ
        elif min_distance <= 10:
            return 1.5  # –°—Ä–µ–¥–Ω–µ
        else:
            return 1.0  # –î–∞–ª–µ–∫–æ
    
    def _calculate_position_bonus(self, query_words: List[str], word_positions: Dict[str, List[int]], total_words: int) -> float:
        """–ë–æ–Ω—É—Å –∑–∞ –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (–Ω–∞—á–∞–ª–æ –≤–∞–∂–Ω–µ–µ)"""
        if not word_positions or total_words == 0:
            return 1.0
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—Ä–µ–¥–Ω—é—é –ø–æ–∑–∏—Ü–∏—é –≤—Å–µ—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
        all_positions = []
        for word in query_words:
            if word in word_positions:
                all_positions.extend(word_positions[word])
        
        if not all_positions:
            return 1.0
        
        avg_position = sum(all_positions) / len(all_positions)
        relative_position = avg_position / total_words if total_words > 0 else 0.5
        
        # –ë–æ–Ω—É—Å: –Ω–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤–∞–∂–Ω–µ–µ
        if relative_position < 0.1:
            return 2.5  # –í –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        elif relative_position < 0.3:
            return 2.0  # –í –ø–µ—Ä–≤–æ–π —Ç—Ä–µ—Ç–∏
        elif relative_position < 0.5:
            return 1.5  # –í –ø–µ—Ä–≤–æ–π –ø–æ–ª–æ–≤–∏–Ω–µ
        else:
            return 1.0  # –í–æ –≤—Ç–æ—Ä–æ–π –ø–æ–ª–æ–≤–∏–Ω–µ


class ContextBoostScorer:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    
    def calculate_boost(self, query_analysis: Dict[str, Any], document: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —É—Å–∏–ª–µ–Ω–∏—è"""
        boost = 1.0
        intent = query_analysis.get('intent', '')
        entities = query_analysis.get('entities', {})
        
        # 1. –¢–∏–ø –∑–≤–æ–Ω–∫–∞
        call_types = entities.get('call_types', [])
        if call_types:
            doc_call_type = document.get('call_type', '')
            if doc_call_type in call_types:
                boost *= 5.0
        
        # 2. –ù–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–∞
        if '–Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ' in intent.lower() or '–Ω–µ–¥–æ–≤–æ–ª—å' in intent.lower():
            if document.get('problem_call_has', False):
                boost *= 4.0
            if document.get('qa_critical_violation', False):
                boost *= 3.0
            if document.get('no_go_count', 0) > 0:
                boost *= 2.0
        
        # 3. –ü—Ä–æ–±–ª–µ–º—ã —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º
        if '–æ–ø–µ—Ä–∞—Ç–æ—Ä' in intent.lower():
            operator_name = query_analysis.get('entities', {}).get('operators', [])
            if operator_name:
                doc_operator = document.get('operator_name', '')
                if any(op.lower() in doc_operator.lower() for op in operator_name):
                    boost *= 3.0
        
        # 4. –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —ç–º–æ—Ü–∏–∏
        if '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ_—ç–º–æ—Ü–∏–∏' in intent:
            if document.get('empathy_count', 0) > 0:
                boost *= 2.5
            if document.get('qa_total_score', 0) >= 80:
                boost *= 1.5
        
        # 5. –ü—Ä–æ–¥–∞–∂–∏
        if '–ø—Ä–æ–¥–∞–∂–∏' in intent:
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–¥–∞–∂
            pass
        
        return boost


class ExactMatchScorer:
    """–ü–æ–¥—Å—á–µ—Ç —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    
    def score(self, query: str, text: str) -> float:
        """–ü–æ–¥—Å—á–µ—Ç –±–æ–Ω—É—Å–∞ –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"""
        query_lower = query.lower().strip()
        text_lower = text.lower()
        
        # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤—Å–µ–π —Ñ—Ä–∞–∑—ã
        if query_lower in text_lower:
            return 10.0
        
        # 2. –í—Å–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        query_words = set(query_lower.split())
        text_words = set(re.findall(r'\w+', text_lower))
        
        if query_words.issubset(text_words):
            return 5.0
        
        # 3. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–ª–æ–≤ (>= 80%)
        match_ratio = len(query_words & text_words) / len(query_words) if query_words else 0
        if match_ratio >= 0.8:
            return 2.0
        elif match_ratio >= 0.6:
            return 1.0
        
        return 0.0


class HybridSearchEngine:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    """
    
    def __init__(self, elasticsearch_url: str, llm_url: str, embedding_model=None):
        self.es = Elasticsearch([elasticsearch_url])
        self.llm_url = llm_url
        self.embedding_model = embedding_model
        self.index_name = "call_dialogues"
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.query_analyzer = QueryAnalyzer(llm_url)
        self.keyword_density_scorer = KeywordDensityScorer()
        self.context_boost_scorer = ContextBoostScorer()
        self.exact_match_scorer = ExactMatchScorer()
        
        # –í–µ—Å–∞ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        self.weights = {
            'bm25': 0.30,
            'semantic': 0.25,
            'keyword_density': 0.25,
            'exact_match': 0.15,
            'context_boost': 0.08,
            'proximity_bonus': 0.05,
            'position_bonus': 0.02
        }
    
    def search(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞
        
        –ü—Ä–æ—Ü–µ—Å—Å:
        1. –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        2. –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ (BM25 + Semantic)
        3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        4. –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
        5. –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
        """
        logger.info(f"üîç –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: '{query}'")
        
        # Stage 1: Query Understanding
        query_analysis = self.query_analyzer.analyze(query)
        logger.info(f"üìä –ê–Ω–∞–ª–∏–∑: intent={query_analysis['intent']}, keywords={query_analysis['keywords']}")
        
        # Stage 2: Multi-Stage Retrieval
        candidates = self._multi_stage_retrieval(query, query_analysis)
        logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates)}")
        
        # Stage 3: Multi-Factor Scoring
        scored_candidates = self._multi_factor_scoring(query, query_analysis, candidates)
        
        # Stage 4: Final Ranking
        ranked = sorted(scored_candidates, key=lambda x: x['final_score'], reverse=True)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for item in ranked[:limit]:
            # –í–∫–ª—é—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            source_data = item.get('_source_data', {})
            if not source_data:
                # –ï—Å–ª–∏ –Ω–µ—Ç _source_data, —Å–æ–∑–¥–∞–µ–º –∏–∑ item
                source_data = {
                    'timestamp': '',
                    'operator_phone': '',
                    'customer_phone': '',
                    'topic_categories': [],
                    'brands': [],
                    'models': [],
                    'text_full': item.get('text_full', ''),
                    'text_clean_full': '',
                    'qa_max_total': 0,
                    'reglament_coverage': 0,
                    'reglament_required': 0,
                    'reglament_passed_all': False,
                    'empathy_count': item.get('empathy_count', 0),
                    'no_go_count': item.get('no_go_count', 0),
                    'dialogue_segments': [],
                    'emotion_meta': {}
                }
            
            results.append({
                "call_id": item['call_id'],
                "call_type": item.get('call_type', ''),
                "operator_name": item.get('operator_name', ''),
                "qa_total_score": item.get('qa_total_score', 0),
                "qa_critical_violation": item.get('qa_critical_violation', False),
                "tags": item.get('tags', []),
                "text_summary": item.get('text_summary', ''),
                "semantic_score": item['final_score'],
                "score_breakdown": item.get('score_breakdown', {}),
                "relevance_reason": self._generate_relevance_reason(item),
                "_source_data": source_data
            })
        
        return {
            "results": results,
            "total": len(ranked),
            "query": query,
            "query_analysis": query_analysis
        }
    
    def _multi_stage_retrieval(self, query: str, query_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        all_candidates = []
        candidate_ids = set()
        
        # Stage 2a: BM25 Search (500 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
        bm25_candidates = self._bm25_search(query, query_analysis, limit=500)
        for cand in bm25_candidates:
            if cand['call_id'] not in candidate_ids:
                cand['source'] = 'bm25'
                all_candidates.append(cand)
                candidate_ids.add(cand['call_id'])
        
        # Stage 2b: Semantic Search (100 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
        if self.embedding_model:
            semantic_candidates = self._semantic_search(query, query_analysis, limit=100)
            for cand in semantic_candidates:
                if cand['call_id'] not in candidate_ids:
                    cand['source'] = 'semantic'
                    all_candidates.append(cand)
                    candidate_ids.add(cand['call_id'])
                else:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç
                    for existing in all_candidates:
                        if existing['call_id'] == cand['call_id']:
                            existing['semantic_score'] = cand.get('semantic_score', 0)
                            existing['source'] = 'both'
                            break
        
        logger.info(f"üì¶ –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {len(all_candidates)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        return all_candidates
    
    def _bm25_search(self, query: str, query_analysis: Dict[str, Any], limit: int = 500) -> List[Dict[str, Any]]:
        """BM25 –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Elasticsearch"""
        expanded_query = query_analysis.get('expanded_query', query)
        concepts = query_analysis.get('concepts', [])
        
        # –°—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å
        should_clauses = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫
        should_clauses.append({
            "multi_match": {
                "query": expanded_query,
                "fields": ["text_full^3", "text_clean_full^2", "text_summary^1", "tags^2"],
                "type": "best_fields",
                "fuzziness": "AUTO"
            }
        })
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º
        if concepts:
            should_clauses.append({
                "multi_match": {
                    "query": " ".join(concepts),
                    "fields": ["text_full^2", "text_summary^1"],
                    "type": "best_fields"
                }
            })
        
        # –§–∏–ª—å—Ç—Ä—ã
        must_clauses = []
        entities = query_analysis.get('entities', {})
        
        if entities.get('call_types'):
            must_clauses.append({
                "terms": {"call_type": entities['call_types']}
            })
        
        search_body = {
            "query": {
                "bool": {
                    "should": should_clauses,
                    "must": must_clauses if must_clauses else None,
                    "minimum_should_match": 1
                }
            },
            "size": limit
        }
        
        try:
            response = self.es.search(index=self.index_name, body=search_body)
            candidates = []
            
            for hit in response['hits']['hits']:
                source = hit['_source']
                candidates.append({
                    'call_id': source['call_id'],
                    'call_type': source.get('call_type', ''),
                    'operator_name': source.get('operator_name', ''),
                    'qa_total_score': source.get('qa_total_score', 0),
                    'qa_critical_violation': source.get('qa_critical_violation', False),
                    'problem_call_has': source.get('problem_call_has', False),
                    'empathy_count': source.get('empathy_count', 0),
                    'no_go_count': source.get('no_go_count', 0),
                    'tags': source.get('tags', []),
                    'text_full': source.get('text_full', ''),
                    'text_summary': source.get('text_summary', ''),
                    'bm25_score': hit['_score'],
                    '_source_data': source
                })
            
            return candidates
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ BM25 –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _semantic_search(self, query: str, query_analysis: Dict[str, Any], limit: int = 100) -> List[Dict[str, Any]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å embeddings"""
        if not self.embedding_model:
            return []
        
        try:
            expanded_query = query_analysis.get('expanded_query', query)
            query_embedding = self.embedding_model.encode(expanded_query, convert_to_numpy=True)
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            search_body = {
                "query": {
                    "multi_match": {
                        "query": expanded_query,
                        "fields": ["text_full", "text_summary"],
                        "type": "best_fields"
                    }
                },
                "size": limit
            }
            
            response = self.es.search(index=self.index_name, body=search_body)
            candidates = []
            
            for hit in response['hits']['hits']:
                source = hit['_source']
                text = f"{source.get('text_summary', '')} {source.get('text_full', '')[:500]}"
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
                doc_embedding = self.embedding_model.encode(text, convert_to_numpy=True)
                
                # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                cosine_sim = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding) + 1e-8
                )
                
                candidates.append({
                    'call_id': source['call_id'],
                    'call_type': source.get('call_type', ''),
                    'operator_name': source.get('operator_name', ''),
                    'qa_total_score': source.get('qa_total_score', 0),
                    'qa_critical_violation': source.get('qa_critical_violation', False),
                    'problem_call_has': source.get('problem_call_has', False),
                    'empathy_count': source.get('empathy_count', 0),
                    'no_go_count': source.get('no_go_count', 0),
                    'tags': source.get('tags', []),
                    'text_full': source.get('text_full', ''),
                    'text_summary': source.get('text_summary', ''),
                    'semantic_score': float(cosine_sim),
                    '_source_data': source
                })
            
            return candidates
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _multi_factor_scoring(self, query: str, query_analysis: Dict[str, Any], candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç –æ—á–∫–æ–≤"""
        scored = []
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è BM25 scores
        bm25_scores = [c.get('bm25_score', 0) for c in candidates if 'bm25_score' in c]
        max_bm25 = max(bm25_scores) if bm25_scores else 1.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Semantic scores
        semantic_scores = [c.get('semantic_score', 0) for c in candidates if 'semantic_score' in c]
        max_semantic = max(semantic_scores) if semantic_scores else 1.0
        
        for candidate in candidates:
            scores = {}
            
            # 1. BM25 Score (0.30)
            bm25 = candidate.get('bm25_score', 0)
            scores['bm25'] = (bm25 / max_bm25) * 100 if max_bm25 > 0 else bm25 * 10  # Fallback –µ—Å–ª–∏ –Ω–µ—Ç max
            
            # 2. Semantic Score (0.25)
            semantic = candidate.get('semantic_score', 0)
            if max_semantic > 0:
                scores['semantic'] = (semantic / max_semantic) * 100
            else:
                scores['semantic'] = semantic * 100  # Fallback
            
            # 3. Keyword Density (0.25)
            query_words = query_analysis.get('keywords', [])
            text = candidate.get('text_full', '') or candidate.get('text_summary', '')
            density_result = self.keyword_density_scorer.score(query_words, text)
            scores['keyword_density'] = density_result['density_score'] * 20  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å
            
            # 4. Exact Match (0.15)
            exact = self.exact_match_scorer.score(query, text)
            scores['exact_match'] = exact
            
            # 5. Context Boost (0.08)
            context_boost = self.context_boost_scorer.calculate_boost(query_analysis, candidate)
            scores['context_boost'] = (context_boost - 1.0) * 20  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            
            # 6. Proximity Bonus (–∏–∑ Keyword Density)
            scores['proximity_bonus'] = (density_result['proximity_bonus'] - 1.0) * 10
            
            # 7. Position Bonus (–∏–∑ Keyword Density)
            scores['position_bonus'] = (density_result['position_bonus'] - 1.0) * 10
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
            final_score = (
                scores['bm25'] * self.weights['bm25'] +
                scores['semantic'] * self.weights['semantic'] +
                scores['keyword_density'] * self.weights['keyword_density'] +
                scores['exact_match'] * self.weights['exact_match'] +
                scores['context_boost'] * self.weights['context_boost'] +
                scores['proximity_bonus'] * self.weights['proximity_bonus'] +
                scores['position_bonus'] * self.weights['position_bonus']
            )
            
            candidate['final_score'] = final_score
            candidate['score_breakdown'] = scores
            scored.append(candidate)
        
        return scored
    
    def _generate_relevance_reason(self, item: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        breakdown = item.get('score_breakdown', {})
        reasons = []
        
        if breakdown.get('exact_match', 0) > 5:
            reasons.append("—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ")
        if breakdown.get('context_boost', 0) > 2:
            reasons.append("–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ")
        if breakdown.get('keyword_density', 0) > 5:
            reasons.append("–≤—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        if breakdown.get('semantic', 0) > 50:
            reasons.append("—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
        
        return ", ".join(reasons) if reasons else "—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–∞–∫—Ç–æ—Ä–æ–≤"
    
    def update_weights(self, new_weights: Dict[str, float]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        self.weights.update(new_weights)
        logger.info(f"‚úÖ –í–µ—Å–∞ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {self.weights}")

