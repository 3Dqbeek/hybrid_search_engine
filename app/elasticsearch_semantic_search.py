import os
import json
import requests
from typing import List, Dict, Any, Optional
from elasticsearch import Elasticsearch
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ElasticsearchSemanticSearch:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ Elasticsearch –±–µ–∑ ML –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, elasticsearch_url: str, llm_url: str):
        self.es = Elasticsearch([elasticsearch_url])
        self.llm_url = llm_url
        self.index_name = "call_dialogues"
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π
        self.semantic_expansions = {
            # –≠–º–æ—Ü–∏–∏ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            "–≥—Ä—É–±—ã–π": ["–Ω–µ–≤–µ–∂–ª–∏–≤—ã–π", "—Ö–∞–º—Å–∫–∏–π", "–Ω–µ—É—á—Ç–∏–≤—ã–π", "—Ä–µ–∑–∫–∏–π", "–∂–µ—Å—Ç–∫–∏–π", "–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π"],
            "–Ω–µ–≤–µ–∂–ª–∏–≤—ã–π": ["–≥—Ä—É–±—ã–π", "—Ö–∞–º—Å–∫–∏–π", "–Ω–µ—É—á—Ç–∏–≤—ã–π", "–Ω–µ—É–≤–∞–∂–∏—Ç–µ–ª—å–Ω—ã–π", "–¥–µ—Ä–∑–∫–∏–π"],
            "—Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω": ["–æ–≥–æ—Ä—á–µ–Ω", "–ø–µ—á–∞–ª–µ–Ω", "–≥—Ä—É—Å—Ç–µ–Ω", "–Ω–µ–¥–æ–≤–æ–ª–µ–Ω", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω"],
            "–∑–ª–æ–π": ["–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π", "—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–Ω—ã–π", "–Ω–µ–¥–æ–≤–æ–ª—å–Ω—ã–π", "–≤–æ–∑–º—É—â–µ–Ω–Ω—ã–π", "—è—Ä–æ—Å—Ç–Ω—ã–π"],
            "–¥–æ–≤–æ–ª—å–Ω—ã–π": ["—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω—ã–π", "—Å—á–∞—Å—Ç–ª–∏–≤—ã–π", "—Ä–∞–¥–æ—Å—Ç–Ω—ã–π", "–ø—Ä–∏—è—Ç–Ω–æ —É–¥–∏–≤–ª–µ–Ω–Ω—ã–π"],
            
            # –ü—Ä–æ–±–ª–µ–º—ã –∏ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–∏
            "–ø—Ä–æ–±–ª–µ–º–∞": ["–Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å", "–ø–æ–ª–æ–º–∫–∞", "—Å–±–æ–π", "–æ—à–∏–±–∫–∞", "–Ω–µ–¥–æ—Ä–∞–±–æ—Ç–∫–∞"],
            "—Å–ª–æ–º–∞–ª—Å—è": ["–Ω–µ–∏—Å–ø—Ä–∞–≤–µ–Ω", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø–æ–ª–æ–º–∫–∞", "—Å–ª–æ–º–∞–Ω", "–≤—ã—à–µ–ª –∏–∑ —Å—Ç—Ä–æ—è"],
            "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç": ["—Å–ª–æ–º–∞–ª—Å—è", "–Ω–µ–∏—Å–ø—Ä–∞–≤–µ–Ω", "–Ω–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç", "–Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç"],
            
            # –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã
            "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ": ["–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "–¥–æ–±—Ä–æ—Å–æ–≤–µ—Å—Ç–Ω–æ", "—Ç—â–∞—Ç–µ–ª—å–Ω–æ"],
            "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ": ["–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ", "—É–º–µ–ª–æ", "–º–∞—Å—Ç–µ—Ä—Å–∫–∏"],
            "—Ö–æ—Ä–æ—à–æ": ["–æ—Ç–ª–∏—á–Ω–æ", "–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ", "–¥–æ–±—Ä–æ—Å–æ–≤–µ—Å—Ç–Ω–æ", "—É—Å–ø–µ—à–Ω–æ"],
            
            # –ù–∞—Ä—É—à–µ–Ω–∏—è –∏ –ø—Ä–æ–±–ª–µ–º—ã
            "–Ω–∞—Ä—É—à–µ–Ω–∏–µ": ["–æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ", "–Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ", "–æ—à–∏–±–∫–∞", "–ø—Ä–æ–±–ª–µ–º–∞", "—Å–±–æ–π"],
            "—Å–∫—Ä–∏–ø—Ç": ["—Å—Ü–µ–Ω–∞—Ä–∏–π", "–∞–ª–≥–æ—Ä–∏—Ç–º", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"],
            "–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ": ["–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ", "–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å"],
            "—ç–º–ø–∞—Ç–∏—è": ["–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "—Å–æ—á—É–≤—Å—Ç–≤–∏–µ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∫–ª–∏–µ–Ω—Ç—É"],
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
            "–∞–ø–ø–∞—Ä–∞—Ç": ["—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "–ø—Ä–∏–±–æ—Ä", "–º–∞—à–∏–Ω–∞", "–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ"],
            "–º–æ—Ç–æ—Ä": ["–¥–≤–∏–≥–∞—Ç–µ–ª—å", "–ø—Ä–∏–≤–æ–¥", "–º–µ—Ö–∞–Ω–∏–∑–º"],
            "–∑–∞–ø—á–∞—Å—Ç–∏": ["–¥–µ—Ç–∞–ª–∏", "–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã", "—ç–ª–µ–º–µ–Ω—Ç—ã"],
            
            # –î–µ–π—Å—Ç–≤–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞
            "–ø–æ–º–æ—â—å": ["–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è", "—Å–æ–¥–µ–π—Å—Ç–≤–∏–µ", "–ø–æ–º–æ—â—å"],
            "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è": ["—Å–æ–≤–µ—Ç", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–ø–æ–º–æ—â—å"],
            "—Ä–µ—à–µ–Ω–∏–µ": ["–æ—Ç–≤–µ—Ç", "–≤—ã—Ö–æ–¥", "—Å–ø–æ—Å–æ–±", "–º–µ—Ç–æ–¥"],
        }
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.context_rules = {
            "–ø—Ä–æ–±–ª–µ–º—ã_–æ–ø–µ—Ä–∞—Ç–æ—Ä–∞": {
                "keywords": ["–≥—Ä—É–±—ã–π", "–Ω–µ–≤–µ–∂–ª–∏–≤—ã–π", "—Ö–∞–º—Å–∫–∏–π", "–ø—Ä–æ–±–ª–µ–º–∞", "–Ω–∞—Ä—É—à–µ–Ω–∏–µ", "–æ—à–∏–±–∫–∞"],
                "boost_fields": ["qa_total_score", "qa_critical_violation"],
                "boost_value": -1,  # –ù–∏–∑–∫–∏–µ –±–∞–ª–ª—ã = –ø—Ä–æ–±–ª–µ–º—ã
                "tags": ["–ù–∞—Ä—É—à–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞", "–ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã", "Dead air"]
            },
            "–∫–∞—á–µ—Å—Ç–≤–æ_—Ä–∞–±–æ—Ç—ã": {
                "keywords": ["–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ", "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ", "—É—Å–ø–µ—à–Ω–æ"],
                "boost_fields": ["qa_total_score"],
                "boost_value": 1,  # –í—ã—Å–æ–∫–∏–µ –±–∞–ª–ª—ã = –∫–∞—á–µ—Å—Ç–≤–æ
                "tags": ["–î–æ–ø—Ä–æ–¥–∞–∂–∞", "–≠–º–ø–∞—Ç–∏—è"]
            },
            "—ç–º–æ—Ü–∏–∏_–∫–ª–∏–µ–Ω—Ç–∞": {
                "keywords": ["—Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω", "–∑–ª–æ–π", "–¥–æ–≤–æ–ª—å–Ω—ã–π", "—Å—á–∞—Å—Ç–ª–∏–≤—ã–π", "–Ω–µ–¥–æ–≤–æ–ª—å–Ω—ã–π"],
                "boost_fields": ["empathy_count"],
                "boost_value": -1,  # –ù–∏–∑–∫–∞—è —ç–º–ø–∞—Ç–∏—è = –ø—Ä–æ–±–ª–µ–º—ã
                "tags": ["–≠–º–ø–∞—Ç–∏—è", "Dead air"]
            }
        }
    
    def expand_query_semantically(self, query: str) -> str:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
        expanded_terms = []
        query_lower = query.lower()
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ
        for word in query_lower.split():
            if word in self.semantic_expansions:
                expanded_terms.extend(self.semantic_expansions[word])
            else:
                expanded_terms.append(word)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        for context, rules in self.context_rules.items():
            if any(keyword in query_lower for keyword in rules["keywords"]):
                expanded_terms.extend(rules["tags"])
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        unique_terms = list(set(expanded_terms))
        expanded_query = f"{query} {' '.join(unique_terms)}"
        
        logger.info(f"–ó–∞–ø—Ä–æ—Å —Ä–∞—Å—à–∏—Ä–µ–Ω: '{query}' -> '{expanded_query}'")
        return expanded_query
    
    def expand_query_with_llm(self, query: str) -> str:
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é LLM –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º LLM
        try:
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            prompt = f"""
–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –¥–∏–∞–ª–æ–≥–æ–≤ –∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä–∞: "{query}"

–†–∞—Å–ø–∏—à–∏ —ç—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º:
- –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç "–ø–æ–∫–∞–∂–∏ –¥–∏–∞–ª–æ–≥–∏" - –∏—â–∏ –ø–æ –ø–æ–ª—è–º text_full, text_summary
- –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç "–≤—Ö–æ–¥—è—â–∏–µ" - –∏—â–∏ –ø–æ call_type
- –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ - —Ä–∞—Å—à–∏—Ä—å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏

–¢–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞, —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.
"""
            
            response = requests.post(
                f"{self.llm_url}/v1/chat/completions",
                json={
                    "model": "qwen/qwen3-coder-30b",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 100,
                    "temperature": 0.1
                },
                timeout=5
            )
            
            if response.status_code == 200:
                result = response.json()
                expanded_query = result['choices'][0]['message']['content'].strip()
                logger.info(f"‚úÖ –ó–∞–ø—Ä–æ—Å —Ä–∞—Å—à–∏—Ä–µ–Ω LLM: {expanded_query}")
                return expanded_query
            else:
                raise Exception(f"LLM status: {response.status_code}")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ")
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–π
            return self._smart_expand_query(query)
    
    def _smart_expand_query(self, query: str) -> str:
        """–£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        query_lower = query.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
        # –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç "–ø–æ–∫–∞–∂–∏ –¥–∏–∞–ª–æ–≥–∏" - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–æ–≤–æ "–¥–∏–∞–ª–æ–≥–∏", —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö
        if "–ø–æ–∫–∞–∂–∏" in query_lower or "–Ω–∞–π–¥–∏" in query_lower:
            # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö
            key_words = []
            for word in query_lower.split():
                if word not in ["–ø–æ–∫–∞–∂–∏", "–Ω–∞–π–¥–∏", "–¥–∏–∞–ª–æ–≥–∏", "–∑–≤–æ–Ω–∫–∏", "—Ä–∞–∑–≥–æ–≤–æ—Ä—ã", "–≥–¥–µ", "–∫–æ–≥–¥–∞"]:
                    key_words.append(word)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
                    if word in self.semantic_expansions:
                        key_words.extend(self.semantic_expansions[word][:2])  # –ü–µ—Ä–≤—ã–µ 2 —Å–∏–Ω–æ–Ω–∏–º–∞
            
            expanded_query = " ".join(key_words)
            logger.info(f"ü§ñ –£–º–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{query}' -> '{expanded_query}'")
            return expanded_query
        
        # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ - –ø—Ä–æ—Å—Ç–æ —Ä–∞—Å—à–∏—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        return self.expand_query_semantically(query)
    
    def semantic_search(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ Elasticsearch"""
        
        # 1. –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏
        expanded_query = self.expand_query_with_llm(query)
        
        # 2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        context_type = self._detect_query_context(query)
        context_rules = self.context_rules.get(context_type, {})
        
        # 3. –°—Ç—Ä–æ–∏–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        filters = []
        query_lower = query.lower()
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∑–≤–æ–Ω–∫–∞
        if "–≤—Ö–æ–¥—è—â" in query_lower or "–≤—Ö–æ–¥" in query_lower:
            filters.append({"term": {"call_type": "–í—Ö–æ–¥—è—â–∏–π –∑–≤–æ–Ω–æ–∫"}})
        elif "–∏—Å—Ö–æ–¥—è—â" in query_lower or "–∏—Å—Ö–æ–¥" in query_lower:
            filters.append({"term": {"call_type": "–ò—Å—Ö–æ–¥—è—â–∏–π –∑–≤–æ–Ω–æ–∫"}})
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É
        if "–æ–ø–µ—Ä–∞—Ç–æ—Ä" in query_lower:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            operator_filter = {"multi_match": {"query": expanded_query, "fields": ["operator_name"], "type": "phrase"}}
            filters.append(operator_filter)
        
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
                        {
                            "multi_match": {
                                "query": expanded_query,
                                "fields": [
                                    "text_full^3",
                                    "text_clean_full^2", 
                                    "text_summary^1"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        },
                        # –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–∞–º
                        {
                            "multi_match": {
                                "query": query,  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                                "fields": [
                                    "text_full^2",
                                    "text_clean_full^1.5", 
                                    "text_summary^1"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ],
                    "must": filters if filters else None,
                    "minimum_should_match": 1
                }
            },
            "sort": [
                {"_score": {"order": "desc"}}
            ],
            "highlight": {
                "fields": {
                    "text_full": {
                        "fragment_size": 200,
                        "number_of_fragments": 3,
                        "pre_tags": ["<mark class='semantic-match'>"],
                        "post_tags": ["</mark>"]
                    },
                    "text_clean_full": {
                        "fragment_size": 200,
                        "number_of_fragments": 2,
                        "pre_tags": ["<mark class='semantic-match'>"],
                        "post_tags": ["</mark>"]
                    }
                }
            },
            "size": limit
        }
        
        # 5. –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        response = self.es.search(index=self.index_name, body=search_body)
        
        # 6. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for hit in response['hits']['hits']:
            source = hit['_source']
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
            highlighted_text = ""
            if 'highlight' in hit:
                highlights = []
                for field in ['text_full', 'dialogue_segments.text']:
                    if field in hit['highlight']:
                        highlights.extend(hit['highlight'][field])
                highlighted_text = " ".join(highlights) if highlights else source.get('text_summary', '')
            
            result = {
                "call_id": source['call_id'],
                "call_type": source.get('call_type', ''),
                "operator_name": source['operator_name'],
                "qa_total_score": source['qa_total_score'],
                "qa_critical_violation": source['qa_critical_violation'],
                "tags": source['tags'],
                "text_summary": source['text_summary'],
                "highlighted_text": highlighted_text,
                "semantic_score": hit['_score'],
                "relevance_reason": self._explain_relevance(query, source, hit['_score'])
            }
            results.append(result)
        
        return {
            "results": results,
            "total": response['hits']['total']['value'],
            "expanded_query": expanded_query,
            "semantic_features": {
                "query_expansion": True,
                "vector_search": False,  # –ë–µ–∑ ML –º–æ–¥–µ–ª–µ–π
                "llm_used": expanded_query != query,
                "context_type": context_type
            }
        }
    
    def _detect_query_context(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        
        for context, rules in self.context_rules.items():
            if any(keyword in query_lower for keyword in rules["keywords"]):
                return context
        
        return "general"
    
    def _explain_relevance(self, query: str, source: Dict, score: float) -> str:
        """–û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        reasons = []
        
        if source['qa_total_score'] < 50:
            reasons.append("–Ω–∏–∑–∫–∏–π QA –±–∞–ª–ª")
        if source['qa_critical_violation']:
            reasons.append("–∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ")
        if any(tag in source['tags'] for tag in ["–ù–∞—Ä—É—à–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞", "–ó–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã"]):
            reasons.append("–Ω–∞—Ä—É—à–µ–Ω–∏—è –≤ —Ä–∞–±–æ—Ç–µ")
        if source.get('empathy_count', 0) == 0:
            reasons.append("–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —ç–º–ø–∞—Ç–∏–∏")
        
        return f"–†–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω ({score:.2f}): {', '.join(reasons)}" if reasons else f"–†–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω ({score:.2f})"


class ElasticsearchSemanticChat:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ Elasticsearch –±–µ–∑ ML –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, es_url: str, llm_url: str, semantic_search: ElasticsearchSemanticSearch):
        self.es = Elasticsearch([es_url])
        self.llm_url = llm_url
        self.semantic_search = semantic_search
    
    async def process_semantic_query(self, query: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
        
        # 1. –í—ã–ø–æ–ª–Ω—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        search_results = self.semantic_search.semantic_search(query, limit=5)
        
        # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context = self._prepare_context(search_results['results'])
        
        # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
        prompt = f"""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤ –∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {query}

–ù–ê–ô–î–ï–ù–ù–´–ï –î–ò–ê–õ–û–ì–ò:
{context}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏
2. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
3. –£–∫–∞–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤
4. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –ø—Ä–µ–¥–ª–æ–∂–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
5. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤

–û–¢–í–ï–¢:
"""
        
        # 4. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
        try:
            response = requests.post(
                f"{self.llm_url}/v1/chat/completions",
                json={
                    "model": "qwen/qwen3-coder-30b",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 500,
                    "temperature": 0.3
                },
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result['choices'][0]['message']['content'].strip()
                
                return {
                    "type": "semantic_llm",
                    "query": query,
                    "expanded_query": search_results['expanded_query'],
                    "llm_response": llm_response,
                    "sources": search_results['results'],
                    "semantic_features": search_results['semantic_features'],
                    "formatted_response": self._format_llm_response(llm_response, search_results['results'])
                }
            else:
                raise Exception(f"LLM error: {response.status_code}")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ LLM: {e}")
            # Fallback –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É
            return self._fallback_analysis(query, search_results)
    
    def _prepare_context(self, results: List[Dict]) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM"""
        context = ""
        
        for i, result in enumerate(results, 1):
            context += f"\n--- –î–ò–ê–õ–û–ì {i} ---\n"
            context += f"ID: {result['call_id']}\n"
            context += f"–û–ø–µ—Ä–∞—Ç–æ—Ä: {result['operator_name']}\n"
            context += f"QA –±–∞–ª–ª: {result['qa_total_score']}\n"
            context += f"–ü—Ä–æ–±–ª–µ–º—ã: {', '.join(result['tags'])}\n"
            context += f"–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {result['text_summary']}\n"
            
            if result['highlighted_text']:
                context += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã: {result['highlighted_text']}\n"
            
            context += f"–ü—Ä–∏—á–∏–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {result['relevance_reason']}\n"
        
        return context
    
    def _format_llm_response(self, llm_response: str, sources: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ LLM"""
        formatted = f"ü§ñ **–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó:**\n\n{llm_response}\n\n"
        
        if sources:
            formatted += "üìö **–ò–°–¢–û–ß–ù–ò–ö–ò:**\n"
            for i, source in enumerate(sources[:3], 1):
                formatted += f"{i}. **{source['call_id']}** - {source['operator_name']} "
                formatted += f"(QA: {source['qa_total_score']}, –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {source['semantic_score']:.2f})\n"
                formatted += f"   –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(source['tags'])}\n"
        
        return formatted
    
    def _fallback_analysis(self, query: str, search_results: Dict) -> Dict[str, Any]:
        """Fallback –∞–Ω–∞–ª–∏–∑ –±–µ–∑ LLM"""
        results = search_results['results']
        
        if not results:
            return {
                "type": "no_results",
                "formatted_response": f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
            }
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis = f"üîç **–ù–ê–ô–î–ï–ù–û {len(results)} –†–ï–õ–ï–í–ê–ù–¢–ù–´–• –î–ò–ê–õ–û–ì–û–í:**\n\n"
        
        for i, result in enumerate(results[:3], 1):
            analysis += f"**{i}. {result['call_id']}** - {result['operator_name']}\n"
            analysis += f"‚Ä¢ QA –±–∞–ª–ª: {result['qa_total_score']}\n"
            analysis += f"‚Ä¢ –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(result['tags'])}\n"
            analysis += f"‚Ä¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['relevance_reason']}\n"
            
            if result['highlighted_text']:
                analysis += f"‚Ä¢ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç: {result['highlighted_text'][:200]}...\n"
            
            analysis += "\n"
        
        return {
            "type": "fallback_analysis",
            "formatted_response": analysis,
            "sources": results,
            "expanded_query": search_results['expanded_query']
        }
