import os
import json
import requests
import logging
from typing import List, Dict, Any, Optional
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QualitySemanticSearch:
    """–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å Russian embeddings –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏"""
    
    def __init__(self, elasticsearch_url: str, llm_url: str):
        self.es = Elasticsearch([elasticsearch_url])
        self.llm_url = llm_url
        self.index_name = "call_dialogues"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å E5-large –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        local_model_path = "/models/embeddings/intfloat_multilingual-e5-large"
        
        try:
            import os
            if os.path.exists(local_model_path):
                logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º E5-large –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {local_model_path}")
                try:
                    self.embedding_model = SentenceTransformer(local_model_path, device='cpu', trust_remote_code=True)
                    logger.info("‚úÖ E5-large –º–æ–¥–µ–ª—å embeddings –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ E5-large: {e}")
                    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑ trust_remote_code
                    try:
                        import warnings
                        warnings.filterwarnings('ignore')
                        self.embedding_model = SentenceTransformer(local_model_path, device='cpu')
                        logger.info("‚úÖ E5-large –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π")
                    except Exception as e2:
                        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e2}")
                        self.embedding_model = None
            else:
                logger.warning(f"‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {local_model_path}")
                logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM-—É—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ ML –º–æ–¥–µ–ª–∏")
                self.embedding_model = None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ {local_model_path}: {e}")
            logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –Ω–∞ LLM –ø—Ä–æ–º–ø—Ç—ã")
            self.embedding_model = None
    
    def enhance_query_with_llm(self, query: str, query_type: str = "search") -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é LLM —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–æ–º"""
        try:
            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–∞
            enhanced_prompt = f"""
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤ –∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–π –µ–≥–æ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.

–¢–ò–ü –ó–ê–ü–†–û–°–ê: {query_type}
–ò–°–•–û–î–ù–´–ô –ó–ê–ü–†–û–°: "{query}"

–¢–í–û–Ø –ó–ê–î–ê–ß–ê:
1. –û–ø—Ä–µ–¥–µ–ª–∏ –∏—Å—Ç–∏–Ω–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
2. –ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ (–Ω–µ —Å–ª–æ–≤–∞!)
3. –†–∞—Å—à–∏—Ä—å –∑–∞–ø—Ä–æ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ –ø–æ–Ω—è—Ç–∏—è–º–∏
4. –î–æ–±–∞–≤—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä–∞

–ü–†–ò–ú–ï–†–´:
- "–ø–æ–∫–∞–∂–∏ –≤—Ö–æ–¥—è—â–∏–µ" ‚Üí –∫–æ–Ω—Ü–µ–ø—Ü–∏—è: "—Ç–∏–ø –≤—ã–∑–æ–≤–∞", "–≤—Ö–æ–¥", "–∫–ª–∏–µ–Ω—Ç –∑–≤–æ–Ω–∏—Ç"
- "–Ω–µ–¥–æ–≤–æ–ª–µ–Ω –∫–ª–∏–µ–Ω—Ç" ‚Üí –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: "–∂–∞–ª–æ–±–∞", "–Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ", "–ø—Ä–æ–±–ª–µ–º–∞", "–ø—Ä–æ—Ç–µ—Å—Ç", "—Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏"
- "–≥—Ä—É–±—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä" ‚Üí –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: "–Ω–µ–≤–µ–∂–ª–∏–≤–æ—Å—Ç—å", "—Ö–∞–º—Å—Ç–≤–æ", "–≥—Ä—É–±–æ—Å—Ç—å", "–Ω–µ—É–≤–∞–∂–µ–Ω–∏–µ", "–Ω–µ—É—á—Ç–∏–≤–æ—Å—Ç—å"

–û–¢–í–ï–¢:
–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û JSON —Å –ø–æ–ª—è–º–∏:
{{
    "enhanced_query": "—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞",
    "concepts": ["–∫–ª—é—á–µ–≤–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è 1", "–∫–æ–Ω—Ü–µ–ø—Ü–∏—è 2", ...],
    "query_intent": "–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è",
    "search_focus": "–Ω–∞ —á–µ–º —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ –ø–æ–∏—Å–∫–µ"
}}
"""
            
            response = requests.post(
                f"{self.llm_url}/v1/chat/completions",
                json={
                    "model": "qwen/qwen3-coder-30b",
                    "messages": [{"role": "user", "content": enhanced_prompt}],
                    "max_tokens": 300,
                    "temperature": 0.2
                },
                timeout=15
            )
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result['choices'][0]['message']['content'].strip()
                
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                try:
                    # –£–±–∏—Ä–∞–µ–º markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                    if '```json' in llm_response:
                        llm_response = llm_response.split('```json')[1].split('```')[0].strip()
                    elif '```' in llm_response:
                        llm_response = llm_response.split('```')[1].split('```')[0].strip()
                    
                    enhanced_data = json.loads(llm_response)
                    logger.info(f"‚úÖ LLM —É–ª—É—á—à–∏–ª –∑–∞–ø—Ä–æ—Å: {enhanced_data}")
                    return enhanced_data
                except json.JSONDecodeError:
                    logger.warning(f"‚ö†Ô∏è LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –≤–∞–ª–∏–¥–Ω—ã–π JSON, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ—Å—Ç—å")
                    return {
                        "enhanced_query": llm_response,
                        "concepts": query.split(),
                        "query_intent": "general",
                        "search_focus": "general"
                    }
            else:
                logger.warning(f"‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (status: {response.status_code})")
                return self._fallback_enhancement(query)
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            return self._fallback_enhancement(query)
    
    def _fallback_enhancement(self, query: str) -> Dict[str, Any]:
        """Fallback —É–ª—É—á—à–µ–Ω–∏–µ –±–µ–∑ LLM"""
        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
        stop_words = {"–ø–æ–∫–∞–∂–∏", "–Ω–∞–π–¥–∏", "–¥–∏–∞–ª–æ–≥–∏", "–∑–≤–æ–Ω–∫–∏", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–∫–∞–∫"}
        key_words = [w for w in query.lower().split() if w not in stop_words]
        
        return {
            "enhanced_query": " ".join(key_words),
            "concepts": key_words,
            "query_intent": "general",
            "search_focus": "general"
        }
    
    def semantic_search(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º embeddings –∏ LLM"""
        
        # 1. –£–ª—É—á—à–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å –ø–æ–º–æ—â—å—é LLM
        enhanced = self.enhance_query_with_llm(query)
        enhanced_query = enhanced.get("enhanced_query", query)
        concepts = enhanced.get("concepts", [])
        query_intent = enhanced.get("query_intent", "general")
        
        logger.info(f"üîç –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}' ‚Üí '{enhanced_query}'")
        logger.info(f"üéØ –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {concepts}")
        logger.info(f"üí≠ –ù–∞–º–µ—Ä–µ–Ω–∏–µ: {query_intent}")
        
        # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        if self.embedding_model:
            try:
                query_embedding = self.embedding_model.encode(enhanced_query, convert_to_numpy=True)
                logger.info(f"‚úÖ Embedding —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä: {query_embedding.shape}")
                
                # 3. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–µ–º–∞–Ω—Ç–∏–∫–∞ —á–µ—Ä–µ–∑ embeddings + —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º query_string –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
                # LLM —É–∂–µ —Ä–∞—Å—à–∏—Ä–∏–ª –∑–∞–ø—Ä–æ—Å —Å –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏, —Ç–µ–ø–µ—Ä—å –¥–µ–ª–∞–µ–º —É–º–Ω—ã–π –ø–æ–∏—Å–∫
                
                # –°—Ç—Ä–æ–∏–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –æ—Ç LLM
                semantic_query_parts = []
                if concepts:
                    semantic_query_parts.extend(concepts)
                semantic_query_parts.append(enhanced_query)
                
                search_body = {
                    "query": {
                        "bool": {
                            "should": [
                                # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ç–∏–ø–∞–º
                                {"term": {"call_type": "–í—Ö–æ–¥—è—â–∏–π –∑–≤–æ–Ω–æ–∫"}} if "–≤—Ö–æ–¥—è—â" in enhanced_query.lower() else None,
                                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É —Å –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏
                                {
                                    "multi_match": {
                                        "query": " ".join(semantic_query_parts),
                                        "fields": ["text_full^3", "text_clean_full^2", "text_summary^1", "tags^2"],
                                        "type": "best_fields",
                                        "fuzziness": "AUTO"
                                    }
                                },
                                # –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                                {
                                    "match_phrase": {
                                        "text_full": {
                                            "query": enhanced_query,
                                            "boost": 2.0
                                        }
                                    }
                                }
                            ],
                            "minimum_should_match": "75%"
                        }
                    },
                    "size": limit
                }
                
                # –£–±–∏—Ä–∞–µ–º None –∏–∑ —Å–ø–∏—Å–∫–∞
                search_body["query"]["bool"]["should"] = [q for q in search_body["query"]["bool"]["should"] if q is not None]
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è embedding: {e}")
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
                search_body = self._build_fallback_search(enhanced_query, concepts)
        else:
            logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å embeddings –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            search_body = self._build_fallback_search(enhanced_query, concepts)
        
        # 4. –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        try:
            response = self.es.search(index=self.index_name, body=search_body)
            
            # 5. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ
            results = []
            candidates = []
            
            for hit in response['hits']['hits']:
                source = hit['_source']
                candidates.append({
                    "hit": hit,
                    "source": source
                })
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å embedding –º–æ–¥–µ–ª—å, –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
            if self.embedding_model and query_embedding is not None:
                try:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                    candidate_texts = []
                    for cand in candidates:
                        text = f"{cand['source'].get('text_summary', '')} {cand['source'].get('text_full', '')[:500]}"
                        candidate_texts.append(text)
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings
                    candidate_embeddings = self.embedding_model.encode(candidate_texts, convert_to_numpy=True, show_progress_bar=False)
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                    from numpy.linalg import norm
                    for i, (cand, cand_embedding) in enumerate(zip(candidates, candidate_embeddings)):
                        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                        cosine_sim = query_embedding @ cand_embedding / (norm(query_embedding) * norm(cand_embedding) + 1e-8)
                        
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º BM25 score –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π score
                        bm25_score = cand['hit']['_score']
                        semantic_score = float(cosine_sim * 10)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –ø–æ—Ö–æ–∂–µ–π —à–∫–∞–ª–µ
                        combined_score = bm25_score + semantic_score
                        
                        cand['semantic_score'] = combined_score
                        cand['cosine_similarity'] = float(cosine_sim)
                
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
                    for cand in candidates:
                        cand['semantic_score'] = cand['hit']['_score']
                        cand['cosine_similarity'] = 0
            else:
                for cand in candidates:
                    cand['semantic_score'] = cand['hit']['_score']
                    cand['cosine_similarity'] = 0
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ combined score
            candidates.sort(key=lambda x: x['semantic_score'], reverse=True)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for cand in candidates:
                hit = cand['hit']
                source = cand['source']
                
                result = {
                    "call_id": source['call_id'],
                    "call_type": source.get('call_type', ''),
                    "operator_name": source['operator_name'],
                    "qa_total_score": source['qa_total_score'],
                    "qa_critical_violation": source['qa_critical_violation'],
                    "tags": source['tags'],
                    "text_summary": source['text_summary'],
                    "semantic_score": cand['semantic_score'],
                    "cosine_similarity": cand.get('cosine_similarity', 0),
                    "relevance_reason": f"–°–µ–º–∞–Ω—Ç–∏–∫–∞: {cand.get('cosine_similarity', 0):.3f} + BM25: {hit['_score']:.2f}"
                }
                results.append(result)
            
            return {
                "results": results,
                "total": response['hits']['total']['value'],
                "enhanced_query": enhanced_query,
                "concepts": concepts,
                "query_intent": query_intent,
                "semantic_features": {
                    "vector_search": self.embedding_model is not None,
                    "llm_enhancement": True,
                    "embedding_model": "multilingual-e5-base" if self.embedding_model else None
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return {
                "results": [],
                "total": 0,
                "error": str(e)
            }
    
    def _build_fallback_search(self, query: str, concepts: List[str]) -> Dict[str, Any]:
        """Fallback –ø–æ–∏—Å–∫ –±–µ–∑ embeddings"""
        return {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["text_full^3", "text_clean_full^2", "text_summary^1"],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "size": 10
        }

